{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3affffde-ebe7-4f0e-8dbe-ac24d670a9ff",
   "metadata": {},
   "source": [
    "WEEK 2: GREEN CART LTD PROJECT \n",
    "Ivelina Angelova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142f5521-e3a6-4f7c-a03f-382ede57c960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\iveli\n",
      "\n",
      "Files available:\n",
      "['.anaconda', '.bash_history', '.conda', '.condarc', '.continuum', '.gitconfig', '.idlerc', '.ipynb_checkpoints', '.ipython', '.jupyter', '.matplotlib', '.ms-ad', '.python_history', '.virtual_documents', '.vscode', 'anaconda3', 'anaconda_projects', 'AppData', 'Application Data', 'Contacts', 'Cookies', 'CrossDevice', 'customer_signups_clean.csv', 'customer_signups_clean_v2.csv', 'customer_signup_analysis.ipynb.ipynb', 'Documents', 'Downloads', 'dq_categories_before_after.csv', 'dq_missing_table.csv', 'dq_summary_kpis.csv', 'Favorites', 'First  Notebook.ipynb', 'Ivelina_Angelova_Week2_GreenCart_Analysis.ipynb.ipynb', 'Links', 'Local Settings', 'Music', 'My Documents', 'NetHood', 'New folder', 'NTUSER.DAT', 'ntuser.dat.LOG1', 'ntuser.dat.LOG2', 'NTUSER.DAT{fd687f85-9714-11f0-83d5-704d7b6a1ba0}.TM.blf', 'NTUSER.DAT{fd687f85-9714-11f0-83d5-704d7b6a1ba0}.TMContainer00000000000000000001.regtrans-ms', 'NTUSER.DAT{fd687f85-9714-11f0-83d5-704d7b6a1ba0}.TMContainer00000000000000000002.regtrans-ms', 'ntuser.ini', 'OneDrive', 'OneDrive - Planet Education Networks', 'output.csv', 'PrintHood', 'Recent', 'Saved Games', 'Searches', 'SendTo', 'Start Menu', 'Templates', 'Untitled-Copy1.ipynb', 'Untitled.ipynb', 'untitled.py', 'Untitled1.ipynb', 'Untitled2.ipynb', 'Untitled3.ipynb', 'Untitled4.ipynb', 'Untitled5.ipynb', 'Untitled6.ipynb', 'Videos']\n",
      "\n",
      "‚ö†Ô∏è Check file names and locations ‚Äî file(s) not found.\n"
     ]
    }
   ],
   "source": [
    "# === WEEK 2: GREEN CART LTD PROJECT ===\n",
    "\n",
    "# ---------- BASIC SETUP ----------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Display options\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "# ---------- VERIFY WORKING DIRECTORY ----------\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"\\nFiles available:\")\n",
    "print(os.listdir())\n",
    "\n",
    "# ---------- LOAD TEST ----------\n",
    "try:\n",
    "    sales = pd.read_csv(\"sales_data.csv\")\n",
    "    products = pd.read_csv(\"product_info.csv\")\n",
    "    customers = pd.read_csv(\"customer_info.csv\")\n",
    "    print(\"\\n‚úÖ All files loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n‚ö†Ô∏è Check file names and locations ‚Äî file(s) not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a565d29a-6312-40eb-9d89-6bb446117113",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sales_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m sales \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m products \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_info.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m customers \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_info.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sales_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sales = pd.read_csv(\"sales_data.csv\")\n",
    "products = pd.read_csv(\"product_info.csv\")\n",
    "customers = pd.read_csv(\"customer_info.csv\")\n",
    "\n",
    "print(\"‚úÖ All files loaded successfully!\")\n",
    "print(\"Sales shape:\", sales.shape)\n",
    "print(\"Products shape:\", products.shape)\n",
    "print(\"Customers shape:\", customers.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1021fef-dc79-4428-9368-a88242f0dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Option 1: Check if files exist before loading\n",
    "try:\n",
    "    # You can specify the full path to your files if they're in a different directory\n",
    "    # For example: sales = pd.read_csv(\"/path/to/your/sales_data.csv\")\n",
    "    sales = pd.read_csv(\"r:/Users/iveli/OneDrive/Desktop/Week2_GreenCart_Projectsales_data.csv\")\n",
    "    products = pd.read_csv(\"product_info.csv\")\n",
    "    customers = pd.read_csv(\"customer_info.csv\")\n",
    "    \n",
    "    print(\"‚úÖ All files loaded successfully!\")\n",
    "    print(\"Sales shape:\", sales.shape)\n",
    "    print(\"Products shape:\", products.shape)\n",
    "    print(\"Customers shape:\", customers.shape)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(\"Please make sure the CSV files exist in this directory or provide the full path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368c12b-fb60-4193-a459-a38c29d56568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# FIXED PATH ‚Äî add missing backslash\n",
    "os.chdir(r\"C:\\Users\\iveli\\OneDrive\\Desktop\\Week2_GreenCart_Project\")\n",
    "\n",
    "# Confirm\n",
    "print(\" Working directory changed to:\", os.getcwd())\n",
    "print(\"\\nFiles in this folder:\")\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0194f1c-5524-4e36-9dcd-2ee777a3b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sales = pd.read_csv(\"sales_data.csv\")\n",
    "products = pd.read_csv(\"product_info.csv\")\n",
    "customers = pd.read_csv(\"customer_info.csv\")\n",
    "\n",
    "print(\" All files loaded successfully!\")\n",
    "print(\"Sales shape:\", sales.shape)\n",
    "print(\"Products shape:\", products.shape)\n",
    "print(\"Customers shape:\", customers.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d11fa-5fdb-4c5b-afa0-bedcc51f6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir():\n",
    "    if file.endswith(\".csv\"):\n",
    "        print(\"Found CSV:\", file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b3db2-5418-4a9b-890b-0e9219a0aa15",
   "metadata": {},
   "source": [
    "# üßÆ Step 1: Load and Inspect Data\n",
    "\n",
    "### üéØ Goal\n",
    "The purpose of this step is to import, inspect, and understand the three datasets provided by Green Cart Ltd.:\n",
    "- **sales_data.csv**\n",
    "- **product_info.csv**\n",
    "- **customer_info.csv**\n",
    "\n",
    "Before cleaning or merging, we must confirm:\n",
    "- The structure and size of each dataset  \n",
    "- Column names and data types  \n",
    "- Missing values, duplicate records, and inconsistencies  \n",
    "- The validity of key fields such as dates and numeric values\n",
    "\n",
    "### üß≠ Actions\n",
    "1. Load each CSV file into a separate Pandas DataFrame.  \n",
    "2. Standardise all column names to *snake_case* for consistency.  \n",
    "3. Use `.info()`, `.shape`, and `.head()` to preview the structure.  \n",
    "4. Identify missing values and duplicate IDs.  \n",
    "5. Check for inconsistent category labels (e.g., region, loyalty_tier).  \n",
    "6. Preview date columns to detect invalid formats.  \n",
    "7. Validate numeric columns to ensure non-negative and realistic values.  \n",
    "8. Export small ‚Äúaudit‚Äù CSVs (missing counts, numeric checks) for screenshots or the report appendix.\n",
    "\n",
    "### üí° Why This Step Matters\n",
    "Conducting an initial audit ensures data reliability.  \n",
    "By identifying problems early‚Äîsuch as blank regions, mis-cased labels, or invalid dates‚Äîwe can plan targeted cleaning steps in Step 2.  \n",
    "This stage forms the foundation for trustworthy analysis later in the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb90a26-d476-4092-916d-297d8449e316",
   "metadata": {},
   "source": [
    "# üßÆ Step 1: Load and Inspect Data\n",
    "\n",
    "### üéØ Goal\n",
    "The purpose of this step is to import, inspect, and understand the three datasets provided by Green Cart Ltd.:\n",
    "- **sales_data.csv**\n",
    "- **product_info.csv**\n",
    "- **customer_info.csv**\n",
    "\n",
    "Before cleaning or merging, we must confirm:\n",
    "- The structure and size of each dataset  \n",
    "- Column names and data types  \n",
    "- Missing values, duplicate records, and inconsistencies  \n",
    "- The validity of key fields such as dates and numeric values\n",
    "\n",
    "### üß≠ Actions\n",
    "1. Load each CSV file into a separate Pandas DataFrame.  \n",
    "2. Standardise all column names to *snake_case* for consistency.  \n",
    "3. Use `.info()`, `.shape`, and `.head()` to preview the structure.  \n",
    "4. Identify missing values and duplicate IDs.  \n",
    "5. Check for inconsistent category labels (e.g., region, loyalty_tier).  \n",
    "6. Preview date columns to detect invalid formats.  \n",
    "7. Validate numeric columns to ensure non-negative and realistic values.  \n",
    "8. Export small ‚Äúaudit‚Äù CSVs (missing counts, numeric checks) for screenshots or the report appendix.\n",
    "\n",
    "### üí° Why This Step Matters\n",
    "Conducting an initial audit ensures data reliability.  \n",
    "By identifying problems early-such as blank regions, mis-cased labels, or invalid dates-we can plan targeted cleaning steps in Step 2.  \n",
    "This stage forms the foundation for trustworthy analysis later in the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f8cdf-976c-4cee-b859-b7faeb8f6e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 1: LOAD & INSPECT (AUDIT ONLY ‚Äî NON-DESTRUCTIVE) ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Expect: you already loaded these in memory earlier\n",
    "# sales, products, customers\n",
    "\n",
    "def to_snake(df):\n",
    "    df2 = df.copy()\n",
    "    df2.columns = (df2.columns\n",
    "                   .str.strip()\n",
    "                   .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "                   .str.lower())\n",
    "    return df2\n",
    "\n",
    "def audit_missing(df, name):\n",
    "    miss = df.isna().sum().to_frame(\"missing\")\n",
    "    miss[\"pct\"] = (miss[\"missing\"] / len(df) * 100).round(2)\n",
    "    miss.insert(0, \"column\", miss.index)\n",
    "    miss = miss.reset_index(drop=True)\n",
    "    print(f\"\\n--- Missing audit: {name} ---\\n\", miss.sort_values(\"missing\", ascending=False).head(20))\n",
    "    return miss\n",
    "\n",
    "def preview_categories(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            vals = df[c].astype(\"string\").str.strip()\n",
    "            print(f\"\\n{c}: unique sample ->\", sorted(vals.dropna().unique().tolist())[:12])\n",
    "\n",
    "def date_preview(df, col, dayfirst=True):\n",
    "    if col not in df.columns: \n",
    "        return {\"col\": col, \"present\": False}\n",
    "    parsed = pd.to_datetime(df[col], errors=\"coerce\", dayfirst=dayfirst)\n",
    "    invalid = parsed.isna().sum()\n",
    "    return {\"col\": col, \"present\": True, \"rows\": len(df),\n",
    "            \"invalid_dates\": int(invalid),\n",
    "            \"invalid_pct\": round(invalid/len(df)*100,2)}\n",
    "\n",
    "def numeric_checks(df, rules):\n",
    "    findings = []\n",
    "    for col, rule in rules.items():\n",
    "        if col not in df.columns: \n",
    "            findings.append((col, \"missing_col\", 0))\n",
    "            continue\n",
    "        s = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        if rule == \"non_negative\":\n",
    "            bad = (s < 0).sum()\n",
    "            findings.append((col, \"negatives\", int(bad)))\n",
    "        elif rule == \"fraction_0_1\":\n",
    "            bad = ((s < 0) | (s > 1)).sum()\n",
    "            findings.append((col, \"out_of_[0,1]\", int(bad)))\n",
    "    return pd.DataFrame(findings, columns=[\"column\",\"issue\",\"count\"])\n",
    "\n",
    "# 1. Standardise column names (work copies)\n",
    "sales_df    = to_snake(sales)\n",
    "products_df = to_snake(products)\n",
    "customers_df= to_snake(customers)\n",
    "\n",
    "print(\"=== Shapes ===\")\n",
    "print(\"sales:\", sales_df.shape, \"| products:\", products_df.shape, \"| customers:\", customers_df.shape)\n",
    "\n",
    "print(\"\\n=== Dtypes (sales) ===\");    print(sales_df.dtypes)\n",
    "print(\"\\n=== Dtypes (products) ===\"); print(products_df.dtypes)\n",
    "print(\"\\n=== Dtypes (customers) ===\");print(customers_df.dtypes)\n",
    "\n",
    "print(\"\\n=== Head previews ===\")\n",
    "print(\"sales:\\n\", sales_df.head(3))\n",
    "print(\"\\nproducts:\\n\", products_df.head(3))\n",
    "print(\"\\ncustomers:\\n\", customers_df.head(3))\n",
    "\n",
    "# 2Ô∏è. Missing audits\n",
    "miss_sales    = audit_missing(sales_df,    \"sales\")\n",
    "miss_products = audit_missing(products_df, \"products\")\n",
    "miss_customers= audit_missing(customers_df,\"customers\")\n",
    "\n",
    "# 3Ô∏è. Duplicate keys (by expected IDs if present)\n",
    "dup_sales = sales_df.duplicated(subset=[c for c in [\"order_id\"] if c in sales_df.columns]).sum()\n",
    "dup_products = products_df.duplicated(subset=[c for c in [\"product_id\"] if c in products_df.columns]).sum()\n",
    "dup_customers = customers_df.duplicated(subset=[c for c in [\"customer_id\"] if c in customers_df.columns]).sum()\n",
    "print(f\"\\n--- Duplicate keys ---\\nsales(order_id): {dup_sales}\\nproducts(product_id): {dup_products}\\ncustomers(customer_id): {dup_customers}\")\n",
    "\n",
    "# 4Ô∏è. Category previews for inconsistent casing/labels\n",
    "preview_categories(sales_df,    [\"delivery_status\",\"payment_method\",\"region\"])\n",
    "preview_categories(products_df, [\"category\",\"supplier_code\"])\n",
    "preview_categories(customers_df,[\"gender\",\"region\",\"loyalty_tier\"])\n",
    "\n",
    "# 5Ô∏è. Date sanity (parse-preview only)\n",
    "date_checks = []\n",
    "date_checks.append(date_preview(sales_df, \"order_date\"))\n",
    "date_checks.append(date_preview(products_df, \"launch_date\"))\n",
    "date_checks.append(date_preview(customers_df, \"signup_date\"))\n",
    "print(\"\\n--- Date parse preview ---\")\n",
    "for d in date_checks:\n",
    "    print(d)\n",
    "\n",
    "# 6Ô∏è. Numeric sanity\n",
    "num_rules = {\n",
    "    \"quantity\": \"non_negative\",\n",
    "    \"unit_price\": \"non_negative\",\n",
    "    \"discount_applied\": \"fraction_0_1\"\n",
    "}\n",
    "num_sales = numeric_checks(sales_df, num_rules)\n",
    "print(\"\\n--- Numeric sanity (sales) ---\\n\", num_sales)\n",
    "\n",
    "# 7Ô∏è. Optional: export audits for screenshots\n",
    "miss_sales.to_csv(\"w2_audit_missing_sales.csv\", index=False)\n",
    "miss_products.to_csv(\"w2_audit_missing_products.csv\", index=False)\n",
    "miss_customers.to_csv(\"w2_audit_missing_customers.csv\", index=False)\n",
    "num_sales.to_csv(\"w2_audit_numeric_sales.csv\", index=False)\n",
    "print(\"\\nüìÅ Saved audit CSVs: w2_audit_missing_*.csv, w2_audit_numeric_sales.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2400e99-3a33-452e-ba43-3d25bb5094e2",
   "metadata": {},
   "source": [
    "# üßπ Step 2: Clean Each Dataset Individually\n",
    "\n",
    "### üéØ Goal\n",
    "Prepare all three datasets (`sales_data`, `product_info`, and `customer_info`) for merging by ensuring accuracy, consistency, and completeness.\n",
    "\n",
    "### üß≠ Actions\n",
    "For each dataset:\n",
    "1. **Fix text formatting** ‚Äì remove extra spaces and standardise casing  \n",
    "   (e.g., `\" delayed \"` ‚Üí `\"Delayed\"`, `\"gold \"` ‚Üí `\"Gold\"`).\n",
    "2. **Convert date columns** ‚Äì `order_date`, `signup_date`, `launch_date` ‚Üí datetime objects.\n",
    "3. **Handle missing values**  \n",
    "   - Replace blank numerics (e.g., `discount_applied`) with `0.0`  \n",
    "   - Fill text fields (`region`, `category`, `gender`, etc.) with `\"Unknown\"`  \n",
    "   - Drop records only if essential keys (`order_id`, `product_id`, `customer_id`) are missing.\n",
    "4. **Remove duplicates** ‚Äì ensure each key (`order_id`, `product_id`, `customer_id`) is unique.\n",
    "5. **Validate numeric values** ‚Äì ensure `quantity`, `unit_price`, and `discount_applied` are non-negative.\n",
    "6. **Confirm cleaning results** ‚Äì print shapes, missing counts, and unique label samples.\n",
    "\n",
    "### üí° Why This Step Matters\n",
    "Data cleaning ensures the merged dataset is reliable for feature engineering and analysis.  \n",
    "Standardising formats prevents merge mismatches and ensures accurate grouping and aggregation in later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aff5c0-59e0-491b-9954-2ead111ea37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 2: CLEAN EACH DATASET ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Work on copies from Step 1\n",
    "sales_clean    = sales_df.copy()\n",
    "products_clean = products_df.copy()\n",
    "customers_clean= customers_df.copy()\n",
    "\n",
    "# ---------- Helper: text normaliser ----------\n",
    "def norm_text(series):\n",
    "    return (series.astype(\"string\")\n",
    "                  .str.strip()\n",
    "                  .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "                  .str.title())\n",
    "\n",
    "# ---------- 1Ô∏è‚É£ SALES DATA CLEANING ----------\n",
    "print(\"\\n=== Cleaning: SALES DATA ===\")\n",
    "if \"delivery_status\" in sales_clean.columns:\n",
    "    sales_clean[\"delivery_status\"] = norm_text(sales_clean[\"delivery_status\"])\n",
    "if \"payment_method\" in sales_clean.columns:\n",
    "    sales_clean[\"payment_method\"] = norm_text(sales_clean[\"payment_method\"])\n",
    "if \"region\" in sales_clean.columns:\n",
    "    sales_clean[\"region\"] = norm_text(sales_clean[\"region\"])\n",
    "\n",
    "# Convert order_date\n",
    "sales_clean[\"order_date\"] = pd.to_datetime(sales_clean[\"order_date\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "# Replace missing numeric fields\n",
    "for col in [\"quantity\",\"unit_price\",\"discount_applied\"]:\n",
    "    if col in sales_clean.columns:\n",
    "        sales_clean[col] = pd.to_numeric(sales_clean[col], errors=\"coerce\")\n",
    "        if col == \"discount_applied\":\n",
    "            sales_clean[col] = sales_clean[col].fillna(0.0)\n",
    "        else:\n",
    "            sales_clean[col] = sales_clean[col].fillna(0)\n",
    "\n",
    "# Drop invalid/blank key rows\n",
    "if \"order_id\" in sales_clean.columns:\n",
    "    before = len(sales_clean)\n",
    "    sales_clean = sales_clean.dropna(subset=[\"order_id\",\"customer_id\",\"product_id\"])\n",
    "    print(f\"Rows dropped (missing IDs): {before - len(sales_clean)}\")\n",
    "\n",
    "# Remove duplicates\n",
    "sales_clean = sales_clean.drop_duplicates(subset=\"order_id\", keep=\"first\")\n",
    "\n",
    "# Validate numeric columns\n",
    "num_cols = [\"quantity\",\"unit_price\",\"discount_applied\"]\n",
    "for col in num_cols:\n",
    "    if col in sales_clean.columns:\n",
    "        negatives = (sales_clean[col] < 0).sum()\n",
    "        if negatives > 0:\n",
    "            print(f\"{col}: {negatives} negative values corrected to 0\")\n",
    "            sales_clean.loc[sales_clean[col] < 0, col] = 0\n",
    "\n",
    "# ---------- 2Ô∏è‚É£ PRODUCT DATA CLEANING ----------\n",
    "print(\"\\n=== Cleaning: PRODUCT DATA ===\")\n",
    "for col in [\"category\",\"supplier_code\"]:\n",
    "    if col in products_clean.columns:\n",
    "        products_clean[col] = norm_text(products_clean[col])\n",
    "\n",
    "products_clean[\"launch_date\"] = pd.to_datetime(products_clean[\"launch_date\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "# Drop invalid product_id\n",
    "if \"product_id\" in products_clean.columns:\n",
    "    before = len(products_clean)\n",
    "    products_clean = products_clean.dropna(subset=[\"product_id\"])\n",
    "    products_clean = products_clean.drop_duplicates(subset=\"product_id\", keep=\"first\")\n",
    "    print(f\"Rows dropped (invalid product_id): {before - len(products_clean)}\")\n",
    "\n",
    "# Fill missing base_price with median if available\n",
    "if \"base_price\" in products_clean.columns:\n",
    "    median_price = products_clean[\"base_price\"].median()\n",
    "    products_clean[\"base_price\"] = products_clean[\"base_price\"].fillna(median_price)\n",
    "\n",
    "# ---------- 3Ô∏è‚É£ CUSTOMER DATA CLEANING ----------\n",
    "print(\"\\n=== Cleaning: CUSTOMER DATA ===\")\n",
    "for col in [\"gender\",\"region\",\"loyalty_tier\"]:\n",
    "    if col in customers_clean.columns:\n",
    "        customers_clean[col] = norm_text(customers_clean[col])\n",
    "\n",
    "customers_clean[\"signup_date\"] = pd.to_datetime(customers_clean[\"signup_date\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "# Drop invalid customer_id\n",
    "if \"customer_id\" in customers_clean.columns:\n",
    "    before = len(customers_clean)\n",
    "    customers_clean = customers_clean.dropna(subset=[\"customer_id\"])\n",
    "    customers_clean = customers_clean.drop_duplicates(subset=\"customer_id\", keep=\"first\")\n",
    "    print(f\"Rows dropped (invalid customer_id): {before - len(customers_clean)}\")\n",
    "\n",
    "# Fill missing text fields\n",
    "fill_values = {\"region\":\"Unknown\",\"gender\":\"Unspecified\",\"loyalty_tier\":\"Unknown\"}\n",
    "customers_clean = customers_clean.fillna(value=fill_values)\n",
    "\n",
    "# ---------- 4Ô∏è‚É£ POST-CLEAN CHECKS ----------\n",
    "print(\"\\n=== AFTER CLEANING SNAPSHOT ===\")\n",
    "print(\"sales:\", sales_clean.shape)\n",
    "print(\"products:\", products_clean.shape)\n",
    "print(\"customers:\", customers_clean.shape)\n",
    "\n",
    "print(\"\\nRemaining missing values (top 10 cols):\")\n",
    "print(sales_clean.isna().sum().sort_values(ascending=False).head(10))\n",
    "print(products_clean.isna().sum().sort_values(ascending=False).head(10))\n",
    "print(customers_clean.isna().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# ---------- 5Ô∏è‚É£ SAVE CLEAN VERSIONS ----------\n",
    "sales_clean.to_csv(\"sales_data_clean.csv\", index=False)\n",
    "products_clean.to_csv(\"product_info_clean.csv\", index=False)\n",
    "customers_clean.to_csv(\"customer_info_clean.csv\", index=False)\n",
    "print(\"\\n‚úÖ Cleaned files saved: sales_data_clean.csv, product_info_clean.csv, customer_info_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ecd9f1-01e9-4859-8371-ad2b0259dba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "083465f4-ef3b-4f90-8656-302063f8c2ab",
   "metadata": {},
   "source": [
    "Where applicable, SQL queries (via pandasql) will be used to validate joins and generate summary tables. This approach provides transparency and replicates real-world data analysis workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f119558b-7158-46c2-a93d-79efa78f5908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandasql if not installed\n",
    "!pip install -q pandasql\n",
    "\n",
    "# Import it\n",
    "from pandasql import sqldf\n",
    "\n",
    "# Define SQL query helper\n",
    "pysqldf = lambda q: sqldf(q, globals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b32f2-7a20-46d7-99a3-b70d5d1ef9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "SELECT region, COUNT(order_id) AS orders, \n",
    "       SUM(quantity * unit_price * (1 - discount_applied)) AS total_revenue\n",
    "FROM sales_clean\n",
    "GROUP BY region\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "region_revenue = pysqldf(q)\n",
    "region_revenue.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0335c8ff-81a7-4506-8409-47a3e65c38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean datasets\n",
    "sales = pd.read_csv(\"sales_data_clean.csv\")\n",
    "products = pd.read_csv(\"product_info_clean.csv\")\n",
    "customers = pd.read_csv(\"customer_info_clean.csv\")\n",
    "\n",
    "# Register them for SQL\n",
    "q = \"\"\"\n",
    "SELECT s.*, p.category, p.base_price, p.launch_date,\n",
    "       c.email, c.signup_date, c.gender, c.loyalty_tier, c.region AS customer_region\n",
    "FROM sales AS s\n",
    "LEFT JOIN products AS p ON s.product_id = p.product_id\n",
    "LEFT JOIN customers AS c ON s.customer_id = c.customer_id\n",
    "\"\"\"\n",
    "merged_df = pysqldf(q)\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "SELECT STRFTIME('%W', order_date) AS order_week,\n",
    "       region,\n",
    "       ROUND(SUM(quantity * unit_price * (1 - discount_applied)), 2) AS weekly_revenue\n",
    "FROM merged_df\n",
    "GROUP BY order_week, region\n",
    "ORDER BY order_week, region\n",
    "\"\"\"\n",
    "weekly_revenue_sql = pysqldf(q)\n",
    "weekly_revenue_sql.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cca4496-40cd-43ce-a33b-cd418bbf4783",
   "metadata": {},
   "source": [
    "# üîó Step 3: Merge the Datasets\n",
    "\n",
    "### üéØ Goal\n",
    "Combine the cleaned datasets into one master DataFrame that includes sales, product, and customer information.\n",
    "\n",
    "### üß≠ Actions\n",
    "1. Merge `sales_clean` with `products_clean` on **product_id**.  \n",
    "2. Merge the result with `customers_clean` on **customer_id**.  \n",
    "3. Use `how='left'` to preserve all sales transactions (even if product or customer data is missing).  \n",
    "4. Verify the merge by checking row counts, nulls introduced, and unique key integrity.  \n",
    "5. Optionally, cross-check results using SQL queries for validation.\n",
    "\n",
    "### üí° Why This Step Matters\n",
    "Merging datasets allows us to connect customer demographics and product attributes to each sale.  \n",
    "It‚Äôs the foundation for feature engineering and generating business insights across categories, loyalty tiers, and regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c83e15-8cdc-45ca-973f-04390fd47f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0572511c-3dc6-44ca-9e7b-e6d557ed2ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_clean = pd.read_csv(\"sales_data_clean.csv\")\n",
    "products_clean = pd.read_csv(\"product_info_clean.csv\")\n",
    "customers_clean = pd.read_csv(\"customer_info_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a552bb6-7add-4575-b4d3-4824d4c4de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 3: MERGE THE DATASETS ===\n",
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "\n",
    "# ---------- 1Ô∏è Merge using Pandas ----------\n",
    "merged_df = (\n",
    "    sales_clean\n",
    "    .merge(products_clean, on=\"product_id\", how=\"left\", suffixes=(\"_sale\", \"_product\"))\n",
    "    .merge(customers_clean, on=\"customer_id\", how=\"left\", suffixes=(\"\", \"_cust\"))\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Merge complete.\")\n",
    "print(\"Merged shape:\", merged_df.shape)\n",
    "print(\"\\nColumns:\", merged_df.columns.tolist()[:12], \"...\")\n",
    "\n",
    "# Quick validation\n",
    "print(\"\\nNull counts after merge (top 10):\")\n",
    "print(merged_df.isna().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\nUnique order IDs:\", merged_df[\"order_id\"].nunique())\n",
    "print(\"Duplicate order IDs:\", merged_df.duplicated(subset=\"order_id\").sum())\n",
    "\n",
    "# ---------- 2Ô∏è SQL cross-check for join integrity ----------\n",
    "q = \"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT s.order_id) AS total_sales_orders,\n",
    "    COUNT(DISTINCT p.product_id) AS total_products,\n",
    "    COUNT(DISTINCT c.customer_id) AS total_customers,\n",
    "    SUM(CASE WHEN p.product_id IS NULL THEN 1 ELSE 0 END) AS missing_product_links,\n",
    "    SUM(CASE WHEN c.customer_id IS NULL THEN 1 ELSE 0 END) AS missing_customer_links\n",
    "FROM sales_clean AS s\n",
    "LEFT JOIN products_clean AS p ON s.product_id = p.product_id\n",
    "LEFT JOIN customers_clean AS c ON s.customer_id = c.customer_id\n",
    "\"\"\"\n",
    "validation = pysqldf(q)\n",
    "print(\"\\nüîç SQL Validation Summary:\")\n",
    "print(validation)\n",
    "\n",
    "# ---------- 3Ô∏è Save merged version ----------\n",
    "merged_df.to_csv(\"merged_sales_data.csv\", index=False)\n",
    "print(\"\\nüìÅ Saved merged dataset as merged_sales_data.csv\")\n",
    "\n",
    "# Preview\n",
    "merged_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09a120-3ec2-47c6-8e50-89c4f1251af0",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Step 4: Feature Engineering\n",
    "\n",
    "### üéØ Goal\n",
    "Enhance the merged dataset with new analytical features that reveal business insights and prepare the data for visualisation.\n",
    "\n",
    "### üß≠ Actions\n",
    "The following new columns are created:\n",
    "1. **revenue** ‚Äì total revenue per transaction  \n",
    "   `revenue = quantity * unit_price * (1 - discount_applied)`\n",
    "2. **order_week** ‚Äì ISO week number extracted from `order_date`  \n",
    "3. **price_band** ‚Äì categorical price segmentation  \n",
    "   - *Low*: < ¬£15  \n",
    "   - *Medium*: ¬£15‚Äì¬£30  \n",
    "   - *High*: > ¬£30\n",
    "4. **days_to_order** ‚Äì number of days between `launch_date` (product) and `order_date` (sale)  \n",
    "5. **email_domain** ‚Äì extracted domain name from customer email (e.g., `gmail.com`)  \n",
    "6. **is_late** ‚Äì boolean flag if `delivery_status` = ‚ÄúDelayed‚Äù\n",
    "\n",
    "### üí° Why This Step Matters\n",
    "Feature engineering translates raw columns into business-relevant indicators.  \n",
    "These features help identify which products and customers drive performance, how time affects orders, and where delays or discounts influence revenue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce1754-9b86-4aae-8d9d-347abb210567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 4: FEATURE ENGINEERING ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure we‚Äôre using the merged dataset\n",
    "merged_df = pd.read_csv(\"merged_sales_data.csv\")\n",
    "\n",
    "print(\"Starting feature engineering on merged dataset of shape:\", merged_df.shape)\n",
    "\n",
    "# ---------- 1Ô∏è Revenue ----------\n",
    "merged_df[\"revenue\"] = merged_df[\"quantity\"] * merged_df[\"unit_price\"] * (1 - merged_df[\"discount_applied\"])\n",
    "\n",
    "# ---------- 2Ô∏è Order Week ----------\n",
    "merged_df[\"order_date\"] = pd.to_datetime(merged_df[\"order_date\"], errors=\"coerce\")\n",
    "merged_df[\"order_week\"] = merged_df[\"order_date\"].dt.isocalendar().week\n",
    "\n",
    "# ---------- 3Ô∏è Price Band ----------\n",
    "merged_df[\"price_band\"] = pd.cut(\n",
    "    merged_df[\"unit_price\"],\n",
    "    bins=[0, 15, 30, np.inf],\n",
    "    labels=[\"Low\", \"Medium\", \"High\"]\n",
    ")\n",
    "\n",
    "# ---------- 4Ô∏è Days to Order ----------\n",
    "merged_df[\"launch_date\"] = pd.to_datetime(merged_df[\"launch_date\"], errors=\"coerce\")\n",
    "merged_df[\"days_to_order\"] = (merged_df[\"order_date\"] - merged_df[\"launch_date\"]).dt.days\n",
    "\n",
    "# ---------- 5Ô∏è Email Domain ----------\n",
    "if \"email\" in merged_df.columns:\n",
    "    merged_df[\"email_domain\"] = merged_df[\"email\"].astype(str).str.extract(r'@([\\w\\.-]+)')\n",
    "\n",
    "# ---------- 6Ô∏è Is Late ----------\n",
    "if \"delivery_status\" in merged_df.columns:\n",
    "    merged_df[\"delivery_status\"] = merged_df[\"delivery_status\"].astype(str).str.title().str.strip()\n",
    "    merged_df[\"is_late\"] = merged_df[\"delivery_status\"] == \"Delayed\"\n",
    "\n",
    "# ---------- 7Ô∏è Post-creation check ----------\n",
    "print(\"\\n=== Feature Summary ===\")\n",
    "print(merged_df[[\"revenue\", \"order_week\", \"price_band\", \"days_to_order\", \"email_domain\", \"is_late\"]].head())\n",
    "\n",
    "print(\"\\nMissing values in new features:\")\n",
    "print(merged_df[[\"revenue\", \"order_week\", \"price_band\", \"days_to_order\"]].isna().sum())\n",
    "\n",
    "# ---------- 8Ô∏è Save enhanced dataset ----------\n",
    "merged_df.to_csv(\"merged_sales_data_features.csv\", index=False)\n",
    "print(\"\\n‚úÖ Saved enhanced dataset as merged_sales_data_features.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08520a-0052-47ef-b2b0-c67f4fab2b64",
   "metadata": {},
   "source": [
    "# üìä Step 5: Create Summary Tables\n",
    "\n",
    "### üéØ Goal\n",
    "Generate concise analytical tables that reveal patterns in sales, customer behaviour, and delivery performance.  \n",
    "These summaries form the foundation for the visualisations and business answers in later steps.\n",
    "\n",
    "### üß≠ Actions\n",
    "Using `groupby()`, `agg()`, and `pivot_table()`, the following summaries will be created:\n",
    "\n",
    "1. **Weekly Revenue Trends by Region**  \n",
    "   - Total revenue and number of orders per week and region.\n",
    "2. **Product Category Performance**  \n",
    "   - Total revenue, total quantity sold, and average discount by category.\n",
    "3. **Customer Behaviour by Loyalty Tier and Signup Month**  \n",
    "   - Revenue, average discount, and order count by tier and month.\n",
    "4. **Delivery Performance by Region and Price Band**  \n",
    "   - Count of delayed vs delivered orders by region and price band.\n",
    "5. **Preferred Payment Methods by Loyalty Tier**  \n",
    "   - Payment preferences by customer segment.\n",
    "\n",
    "### üí° Why This Step Matters\n",
    "Summary tables simplify the complexity of thousands of transactions into digestible business insights.  \n",
    "They help identify which categories and regions perform best, which customers generate more value, and where operational improvements are needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06521809-2907-4536-bfe1-fa5178cbaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 5: CREATE SUMMARY TABLES ===\n",
    "import pandas as pd\n",
    "\n",
    "# Load enhanced dataset\n",
    "merged_df = pd.read_csv(\"merged_sales_data_features.csv\")\n",
    "\n",
    "print(\"Dataset loaded for summary creation. Shape:\", merged_df.shape)\n",
    "\n",
    "# ---------- 1Ô∏è Weekly Revenue Trends by Region ----------\n",
    "weekly_revenue = (\n",
    "    merged_df.groupby([\"order_week\", \"region\"], as_index=False)\n",
    "    .agg(total_revenue=(\"revenue\", \"sum\"), orders=(\"order_id\", \"count\"))\n",
    "    .sort_values([\"order_week\", \"region\"])\n",
    ")\n",
    "\n",
    "print(\"\\n=== Weekly Revenue by Region (sample) ===\")\n",
    "print(weekly_revenue.head())\n",
    "\n",
    "# ---------- 2Ô∏è Product Category Performance ----------\n",
    "category_perf = (\n",
    "    merged_df.groupby(\"category\", as_index=False)\n",
    "    .agg(total_revenue=(\"revenue\", \"sum\"),\n",
    "         total_quantity=(\"quantity\", \"sum\"),\n",
    "         avg_discount=(\"discount_applied\", \"mean\"))\n",
    "    .sort_values(\"total_revenue\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Product Category Performance (sample) ===\")\n",
    "print(category_perf.head())\n",
    "\n",
    "# ---------- 3Ô∏è Customer Behaviour by Loyalty Tier and Signup Month ----------\n",
    "merged_df[\"signup_date\"] = pd.to_datetime(merged_df[\"signup_date\"], errors=\"coerce\")\n",
    "merged_df[\"signup_month\"] = merged_df[\"signup_date\"].dt.month_name()\n",
    "\n",
    "loyalty_behaviour = (\n",
    "    merged_df.groupby([\"loyalty_tier\", \"signup_month\"], as_index=False)\n",
    "    .agg(revenue=(\"revenue\", \"sum\"),\n",
    "         avg_discount=(\"discount_applied\", \"mean\"),\n",
    "         order_count=(\"order_id\", \"count\"))\n",
    "    .sort_values(\"revenue\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Customer Behaviour by Loyalty Tier & Signup Month (sample) ===\")\n",
    "print(loyalty_behaviour.head())\n",
    "\n",
    "# ---------- 4Ô∏è Delivery Performance by Region and Price Band ----------\n",
    "delivery_perf = (\n",
    "    merged_df.groupby([\"region\", \"price_band\"], as_index=False)\n",
    "    .agg(total_orders=(\"order_id\", \"count\"),\n",
    "         delayed_orders=(\"is_late\", \"sum\"))\n",
    ")\n",
    "delivery_perf[\"delay_rate_%\"] = round(delivery_perf[\"delayed_orders\"] / delivery_perf[\"total_orders\"] * 100, 2)\n",
    "\n",
    "print(\"\\n=== Delivery Performance (sample) ===\")\n",
    "print(delivery_perf.head())\n",
    "\n",
    "# ---------- 5Ô∏è Preferred Payment Methods by Loyalty Tier ----------\n",
    "payment_pref = (\n",
    "    merged_df.groupby([\"loyalty_tier\", \"payment_method\"], as_index=False)\n",
    "    .agg(order_count=(\"order_id\", \"count\"))\n",
    "    .sort_values(\"order_count\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Preferred Payment Methods (sample) ===\")\n",
    "print(payment_pref.head())\n",
    "\n",
    "# ---------- 6Ô∏è Save all summaries ----------\n",
    "weekly_revenue.to_csv(\"summary_weekly_revenue.csv\", index=False)\n",
    "category_perf.to_csv(\"summary_category_perf.csv\", index=False)\n",
    "loyalty_behaviour.to_csv(\"summary_loyalty_behaviour.csv\", index=False)\n",
    "delivery_perf.to_csv(\"summary_delivery_perf.csv\", index=False)\n",
    "payment_pref.to_csv(\"summary_payment_pref.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Saved all summary tables as CSVs in the project folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6996d1-a40c-436f-b3b3-fef6fc5443c0",
   "metadata": {},
   "source": [
    "# üìà Step 6: Visual Exploration\n",
    "\n",
    "### üéØ Goal\n",
    "Create clear, labelled visualisations to help identify key patterns in Green Cart Ltd‚Äôs sales and customer behaviour.\n",
    "\n",
    "### üß≠ Actions\n",
    "Six main visualisations will be created:\n",
    "1. **Line Plot ‚Äì Weekly Revenue Trends by Region**  \n",
    "   Shows how sales performance varies by week and region.\n",
    "2. **Bar Chart ‚Äì Top 5 Product Categories by Revenue**  \n",
    "   Highlights the most profitable product categories.\n",
    "3. **Boxplot ‚Äì Quantity vs Discount by Category**  \n",
    "   Reveals how discounts affect the number of items sold.\n",
    "4. **Heatmap ‚Äì Correlation Between Revenue, Discount, and Quantity**  \n",
    "   Identifies numeric relationships across sales metrics.\n",
    "5. **Countplot ‚Äì Orders by Loyalty Tier (hue = Region)**  \n",
    "   Displays customer distribution across loyalty tiers and regions.\n",
    "6. **Stacked Bar Chart ‚Äì Delivery Status by Price Band**  \n",
    "   Compares delivery performance across different product price ranges.\n",
    "\n",
    "### üí° Why This Step Matters\n",
    "Visualisation transforms data into intuitive insights that can be quickly understood by non-technical stakeholders.  \n",
    "These charts will support the written findings and recommendations in your Week 2 report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa1ece1-58d9-4506-8356-6fea44ac3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 6: VISUAL EXPLORATION ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load enhanced dataset\n",
    "merged_df = pd.read_csv(\"merged_sales_data_features.csv\")\n",
    "\n",
    "# Set consistent visual style\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "# ---------- 1Ô∏è‚É£ Line Plot ‚Äî Weekly Revenue Trends by Region ----------\n",
    "plt.figure(figsize=(10,6))\n",
    "weekly_plot = merged_df.groupby([\"order_week\", \"region\"], as_index=False)[\"revenue\"].sum()\n",
    "sns.lineplot(data=weekly_plot, x=\"order_week\", y=\"revenue\", hue=\"region\", marker=\"o\")\n",
    "plt.title(\"Weekly Revenue Trends by Region\")\n",
    "plt.xlabel(\"Week Number\")\n",
    "plt.ylabel(\"Total Revenue (¬£)\")\n",
    "plt.legend(title=\"Region\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"chart_weekly_revenue_by_region.png\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- 2Ô∏è‚É£ Bar Chart ‚Äî Top 5 Product Categories by Revenue ----------\n",
    "plt.figure(figsize=(8,5))\n",
    "top_categories = merged_df.groupby(\"category\")[\"revenue\"].sum().sort_values(ascending=False).head(5)\n",
    "sns.barplot(x=top_categories.values, y=top_categories.index)\n",
    "plt.title(\"Top 5 Product Categories by Revenue\")\n",
    "plt.xlabel(\"Total Revenue (¬£)\")\n",
    "plt.ylabel(\"Product Category\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"chart_top5_categories.png\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- 3Ô∏è‚É£ Boxplot ‚Äî Quantity vs Discount by Category ----------\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(data=merged_df, x=\"category\", y=\"quantity\", hue=\"price_band\")\n",
    "plt.title(\"Quantity vs Discount Across Categories\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Quantity Sold\")\n",
    "plt.legend(title=\"Price Band\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"chart_boxplot_quantity_discount.png\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- 4Ô∏è‚É£ Heatmap ‚Äî Correlation Between Revenue, Discount, Quantity ----------\n",
    "plt.figure(figsize=(6,5))\n",
    "corr = merged_df[[\"revenue\", \"quantity\", \"discount_applied\"]].corr()\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True)\n",
    "plt.title(\"Correlation Between Revenue, Discount, and Quantity\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"chart_heatmap_correlation.png\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- 5Ô∏è‚É£ Countplot ‚Äî Orders by Loyalty Tier (hue = Region) ----------\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(data=merged_df, x=\"loyalty_tier\", hue=\"region\")\n",
    "plt.title(\"Orders by Loyalty Tier and Region\")\n",
    "plt.xlabel(\"Loyalty Tier\")\n",
    "plt.ylabel(\"Number of Orders\")\n",
    "plt.legend(title=\"Region\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"chart_countplot_loyalty_region.png\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- 6Ô∏è‚É£ Stacked Bar Chart ‚Äî Delivery Status by Price Band ----------\n",
    "plt.figure(figsize=(7,5))\n",
    "delivery_table = pd.crosstab(merged_df[\"price_band\"], merged_df[\"delivery_status\"], normalize=\"index\") * 100\n",
    "delivery_table.plot(kind=\"bar\", stacked=True, colormap=\"Pastel1\", figsize=(8,5))\n",
    "plt.title(\"Delivery Status by Price Band (%)\")\n",
    "plt.xlabel(\"Price Band\")\n",
    "plt.ylabel(\"Percentage of Orders\")\n",
    "plt.legend(title=\"Delivery Status\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"chart_delivery_status_by_priceband.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ All 6 visualisations generated and saved as PNG files:\")\n",
    "print(\"chart_weekly_revenue_by_region.png\")\n",
    "print(\"chart_top5_categories.png\")\n",
    "print(\"chart_boxplot_quantity_discount.png\")\n",
    "print(\"chart_heatmap_correlation.png\")\n",
    "print(\"chart_countplot_loyalty_region.png\")\n",
    "print(\"chart_delivery_status_by_priceband.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fc0195-80b2-4427-919d-f8ac75669e5e",
   "metadata": {},
   "source": [
    "# üß† Step 7: Business Insights & Report Preparation\n",
    "\n",
    "### üéØ Goal\n",
    "Translate the analytical findings into clear, data-backed business insights.  \n",
    "Answer the key business questions from the project brief and prepare evidence for the final PDF report.\n",
    "\n",
    "### üß≠ Business Questions to Answer\n",
    "\n",
    "1. **Which product categories drive the most revenue, and in which regions?**  \n",
    "   Identify top-performing categories and regional patterns.\n",
    "\n",
    "2. **Do discounts lead to more items sold?**  \n",
    "   Examine whether higher discounts correlate with higher quantities.\n",
    "\n",
    "3. **Which loyalty tier generates the most value?**  \n",
    "   Evaluate total revenue and order volume by loyalty tier.\n",
    "\n",
    "4. **Are certain regions struggling with delivery delays?**  \n",
    "   Analyse delay percentages by region and price band.\n",
    "\n",
    "5. **Do customer signup patterns influence purchasing activity?**  \n",
    "   Explore whether signup month or tenure affects order behaviour.\n",
    "\n",
    "### üí° Why This Step Matters\n",
    "This is where analysis turns into business strategy.  \n",
    "By linking sales, customer, and delivery data, we provide insights that help Green Cart Ltd. improve marketing efficiency, strengthen operations, and boost customer engagement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b147c2-8673-48ab-801b-6d89ee3e19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 7: BUSINESS INSIGHTS & REPORT PREPARATION ===\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the fully featured dataset\n",
    "merged_df = pd.read_csv(\"merged_sales_data_features.csv\")\n",
    "\n",
    "# ---------- 1Ô∏è‚É£ Top Product Categories & Regions ----------\n",
    "top_cat_region = (\n",
    "    merged_df.groupby([\"category\", \"region\"], as_index=False)\n",
    "    .agg(total_revenue=(\"revenue\", \"sum\"),\n",
    "         orders=(\"order_id\", \"count\"))\n",
    "    .sort_values(\"total_revenue\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\n=== 1Ô∏è‚É£ Top Product Categories & Regions ===\")\n",
    "print(top_cat_region.head(10))\n",
    "\n",
    "# ---------- 2Ô∏è‚É£ Discounts vs Quantity Sold ----------\n",
    "corr_discount_quantity = merged_df[[\"discount_applied\", \"quantity\"]].corr().iloc[0,1]\n",
    "print(f\"\\n=== 2Ô∏è‚É£ Correlation between Discount and Quantity Sold: {corr_discount_quantity:.2f} ===\")\n",
    "\n",
    "sns.scatterplot(data=merged_df, x=\"discount_applied\", y=\"quantity\", alpha=0.6)\n",
    "plt.title(\"Discount vs Quantity Sold\")\n",
    "plt.xlabel(\"Discount Applied (0‚Äì1)\")\n",
    "plt.ylabel(\"Quantity Sold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"chart_discount_vs_quantity.png\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- 3Ô∏è‚É£ Loyalty Tier Value ----------\n",
    "loyalty_value = (\n",
    "    merged_df.groupby(\"loyalty_tier\", as_index=False)\n",
    "    .agg(total_revenue=(\"revenue\", \"sum\"), orders=(\"order_id\", \"count\"))\n",
    "    .sort_values(\"total_revenue\", ascending=False)\n",
    ")\n",
    "print(\"\\n=== 3Ô∏è‚É£ Loyalty Tier Value ===\")\n",
    "print(loyalty_value)\n",
    "\n",
    "# ---------- 4Ô∏è‚É£ Delivery Delays by Region ----------\n",
    "region_delay = (\n",
    "    merged_df.groupby(\"region\", as_index=False)\n",
    "    .agg(total_orders=(\"order_id\", \"count\"),\n",
    "         delayed_orders=(\"is_late\", \"sum\"))\n",
    ")\n",
    "region_delay[\"delay_rate_%\"] = (region_delay[\"delayed_orders\"] / region_delay[\"total_orders\"] * 100).round(2)\n",
    "print(\"\\n=== 4Ô∏è‚É£ Delivery Delay Rate by Region ===\")\n",
    "print(region_delay.sort_values(\"delay_rate_%\", ascending=False))\n",
    "\n",
    "# ---------- 5Ô∏è‚É£ Signup Month vs Purchasing Activity ----------\n",
    "merged_df[\"signup_date\"] = pd.to_datetime(merged_df[\"signup_date\"], errors=\"coerce\")\n",
    "merged_df[\"signup_month\"] = merged_df[\"signup_date\"].dt.month_name()\n",
    "signup_activity = (\n",
    "    merged_df.groupby(\"signup_month\", as_index=False)\n",
    "    .agg(total_revenue=(\"revenue\", \"sum\"),\n",
    "         avg_quantity=(\"quantity\", \"mean\"),\n",
    "         orders=(\"order_id\", \"count\"))\n",
    "    .sort_values(\"total_revenue\", ascending=False)\n",
    ")\n",
    "print(\"\\n=== 5Ô∏è‚É£ Signup Month vs Purchasing Activity ===\")\n",
    "print(signup_activity)\n",
    "\n",
    "# ---------- SAVE ALL INSIGHT TABLES ----------\n",
    "top_cat_region.to_csv(\"insight_top_categories_regions.csv\", index=False)\n",
    "loyalty_value.to_csv(\"insight_loyalty_value.csv\", index=False)\n",
    "region_delay.to_csv(\"insight_region_delay.csv\", index=False)\n",
    "signup_activity.to_csv(\"insight_signup_activity.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Insight tables saved for reporting:\")\n",
    "print(\"insight_top_categories_regions.csv\")\n",
    "print(\"insight_loyalty_value.csv\")\n",
    "print(\"insight_region_delay.csv\")\n",
    "print(\"insight_signup_activity.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd19821d-8e70-47cf-acd0-7e781170f5af",
   "metadata": {},
   "source": [
    "# üöÄ Step 8: Optional Stretch Tasks (Bonus)\n",
    "\n",
    "### üéØ Goal\n",
    "Add advanced analytical logic to show initiative and deeper understanding of data patterns.\n",
    "\n",
    "### üß≠ Actions\n",
    "1. Use **SQL queries** (via `pandasql`) to extract specific customer segments:\n",
    "   - Customers who signed up in **Q2**\n",
    "   - Placed an order within **14 days**\n",
    "   - Received a **discount > 20 %**\n",
    "\n",
    "2. Apply **MinMaxScaler** to normalise revenue for modelling or fair comparison.\n",
    "\n",
    "3. Flag **underperforming products** that:\n",
    "   - Have low quantities sold  \n",
    "   - High average discounts  \n",
    "   - Frequent delayed deliveries  \n",
    "\n",
    "### üí° Why This Step Matters\n",
    "These tasks demonstrate your ability to go beyond the brief, applying analytical thinking and basic predictive logic.  \n",
    "They can support future business decisions like promotional targeting or product improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d709ac6-3934-4bbf-9514-f5e726686987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 8: OPTIONAL STRETCH TASKS ===\n",
    "from pandasql import sqldf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "\n",
    "# Load data\n",
    "merged_df = pd.read_csv(\"merged_sales_data_features.csv\")\n",
    "\n",
    "# ---------- 1Ô∏è‚É£ Customers who signed up in Q2, ordered within 14 days, and received > 20 % discount ----------\n",
    "q = \"\"\"\n",
    "SELECT DISTINCT customer_id, order_id, signup_date, order_date, discount_applied\n",
    "FROM merged_df\n",
    "WHERE CAST(STRFTIME('%m', signup_date) AS INTEGER) BETWEEN 4 AND 6\n",
    "  AND (JULIANDAY(order_date) - JULIANDAY(signup_date)) <= 14\n",
    "  AND discount_applied > 0.20\n",
    "\"\"\"\n",
    "q2_customers = pysqldf(q)\n",
    "print(\"\\n=== Q2 Customers with early orders & high discounts ===\")\n",
    "print(q2_customers.head())\n",
    "\n",
    "# ---------- 2Ô∏è‚É£ MinMaxScaler to normalise revenue ----------\n",
    "scaler = MinMaxScaler()\n",
    "merged_df[\"revenue_scaled\"] = scaler.fit_transform(merged_df[[\"revenue\"]])\n",
    "print(\"\\nScaled revenue sample:\")\n",
    "print(merged_df[[\"revenue\", \"revenue_scaled\"]].head())\n",
    "\n",
    "# ---------- 3Ô∏è‚É£ Flag underperforming products ----------\n",
    "merged_df[\"underperforming\"] = (\n",
    "    (merged_df[\"quantity\"] < merged_df[\"quantity\"].median()) &\n",
    "    (merged_df[\"discount_applied\"] > 0.20) &\n",
    "    (merged_df[\"is_late\"] == True)\n",
    ")\n",
    "underperf = (\n",
    "    merged_df.groupby(\"category\", as_index=False)\n",
    "    .agg(underperforming_count=(\"underperforming\", \"sum\"),\n",
    "         total_products=(\"product_id\", \"nunique\"))\n",
    ")\n",
    "underperf[\"underperforming_rate_%\"] = round(\n",
    "    underperf[\"underperforming_count\"] / underperf[\"total_products\"] * 100, 2\n",
    ")\n",
    "\n",
    "print(\"\\n=== Underperforming Products by Category ===\")\n",
    "print(underperf.sort_values(\"underperforming_rate_%\", ascending=False))\n",
    "\n",
    "# Save optional outputs\n",
    "q2_customers.to_csv(\"stretch_q2_customers.csv\", index=False)\n",
    "underperf.to_csv(\"stretch_underperforming_products.csv\", index=False)\n",
    "print(\"\\n‚úÖ Optional stretch outputs saved:\")\n",
    "print(\"stretch_q2_customers.csv\")\n",
    "print(\"stretch_underperforming_products.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26458c04-c516-4cfe-b39d-e6f61870aa71",
   "metadata": {},
   "source": [
    "# üèÅ Final Summary & Conclusion\n",
    "\n",
    "### üåø Project: Week 2 ‚Äì Sales & Customer Behaviour Insights ‚Äì Green Cart Ltd.\n",
    "This project analysed Green Cart Ltd.‚Äôs sales, customer, and product data to understand revenue drivers, customer engagement, and operational performance.  \n",
    "Through a structured data analysis process, the datasets were cleaned, merged, enriched with new features, and explored visually to deliver clear, data-driven insights.\n",
    "\n",
    "---\n",
    "\n",
    "### üßπ Data Preparation\n",
    "- Three datasets (`sales_data`, `product_info`, `customer_info`) were cleaned individually to remove duplicates, standardise text, and fix inconsistent formatting.  \n",
    "- Missing values were handled logically ‚Äî numeric columns filled with `0.0`, categorical with `\"Unknown\"`.  \n",
    "- Dates were converted to proper `datetime` objects and verified for consistency.  \n",
    "- Validation ensured all numeric values were non-negative and key IDs were unique.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Feature Engineering\n",
    "Key business-oriented fields were created:\n",
    "- **revenue** = `quantity * unit_price * (1 - discount_applied)`  \n",
    "- **order_week** (ISO week of order)  \n",
    "- **price_band** (Low / Medium / High)  \n",
    "- **days_to_order** (from launch to order)  \n",
    "- **email_domain** (for communication insights)  \n",
    "- **is_late** (flag for delayed deliveries)  \n",
    "These features enabled trend analysis and behavioural segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Key Findings\n",
    "- **Top categories** such as *Home Cleaning* and *Kitchen* generated the most revenue, particularly in the *South* and *North* regions.  \n",
    "- **Discounts** had a mild positive correlation with quantity sold, indicating moderate price sensitivity.  \n",
    "- **Gold loyalty-tier customers** delivered the highest lifetime value.  \n",
    "- **Delivery delays** were most frequent in *Northern* regions and for *High* price-band products.  \n",
    "- **Signup patterns** showed customers joining mid-year (April‚ÄìJune) were the most active purchasers.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Business Recommendations\n",
    "1. **Strengthen top-performing categories** through targeted promotions and sustainable inventory planning.  \n",
    "2. **Reduce delivery delays** by reviewing logistics performance in weaker regions.  \n",
    "3. **Reward Gold-tier customers** with loyalty benefits; design incentives to convert Silver and Bronze users.  \n",
    "4. **Leverage signup trends** by launching referral or seasonal campaigns around Q2.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Optional Insights (Stretch Analysis)\n",
    "- Identified **Q2 customers** who ordered within 14 days and received >20% discounts ‚Äî potential early adopters for remarketing.  \n",
    "- Used `MinMaxScaler` to normalise revenue for fair product comparison.  \n",
    "- Flagged **underperforming products** that combine low quantity, high discount, and frequent delays ‚Äî ideal candidates for review or discontinuation.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Visuals Produced\n",
    "1. Weekly revenue by region  \n",
    "2. Top 5 product categories by revenue  \n",
    "3. Quantity vs discount by category (boxplot)  \n",
    "4. Correlation heatmap (revenue, discount, quantity)  \n",
    "5. Orders by loyalty tier & region (countplot)  \n",
    "6. Delivery status by price band (stacked bar)\n",
    "\n",
    "All visuals were saved as `.png` files and referenced in the final PDF report.\n",
    "\n",
    "---\n",
    "\n",
    "### üßæ Deliverables\n",
    "- **Notebook:** `Ivelina_Angelova_Week2_GreenCart_Analysis.ipynb`  \n",
    "- **Cleaned datasets:** sales, product, customer  \n",
    "- **Merged dataset:** `merged_sales_data_features.csv`  \n",
    "- **Summary & insight tables:** CSVs in project folder  \n",
    "- **6 visualisations:** `.png` format  \n",
    "- **PDF report:** 1,200‚Äì1,500 words with coversheet\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ Reflection\n",
    "This project demonstrated how raw sales data can be transformed into meaningful insights through systematic cleaning, analysis, and visualisation.  \n",
    "The approach followed here,  from validation to storytelling, mirrors real-world data analytics practices and provides a strong foundation for deeper predictive or dashboard-based work in future phases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54230097-3988-401f-bcf9-a96d38bb3d24",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
